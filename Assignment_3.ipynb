{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Assignment Overview: Working with Patient Records and Encounter Notes\n",
    "\n",
    "In this final assignment, we’ll focus on patient records related to COVID-19 encounters. Our task is to analyze, process, and transform the data while applying the concepts we’ve covered throughout this course. Here's a detailed breakdown of the assignment:\n",
    "\n",
    "What Are Encounter Notes?\n",
    "An encounter note is a record that captures details about a patient’s visit with a doctor. It includes both structured and semi-structured information that is crucial for understanding the context of the visit. Here’s what an encounter note typically looks like:\n",
    "\n",
    "```\n",
    "AMBULATORY ENCOUNTER NOTE\n",
    "Date of Service: March 2, 2020 15:45-16:30\n",
    "\n",
    "DEMOGRAPHICS:\n",
    "Name: Jeffrey Greenfelder\n",
    "DOB: 1/16/2005\n",
    "Gender: Male\n",
    "Address: 428 Wiza Glen Unit 91, Springfield, Massachusetts 01104\n",
    "Insurance: Guardian\n",
    "MRN: 055ae6fc-7e18-4a39-8058-64082ca6d515\n",
    "\n",
    "PERTINENT MEDICAL HISTORY:\n",
    "- Obesity \n",
    "\n",
    "Recent Visit: Well child visit (2/23/2020)\n",
    "Immunizations: Influenza vaccine (2/23/2020)\n",
    "\n",
    "Recent Baseline (2/23/2020):\n",
    "Height: 155.0 cm\n",
    "Weight: 81.2 kg\n",
    "BMI: 33.8 kg/m² (99.1th percentile)\n",
    "BP: 123/80 mmHg\n",
    "HR: 92/min\n",
    "RR: 13/min\n",
    "\n",
    "SUBJECTIVE:\n",
    "Adolescent patient presents with multiple symptoms including:\n",
    "- Cough\n",
    "- Sore throat\n",
    "- Severe fatigue\n",
    "- Muscle pain\n",
    "- Joint pain\n",
    "- Fever\n",
    "Never smoker. Symptoms began recently.\n",
    "\n",
    "OBJECTIVE:\n",
    "Vitals:\n",
    "Temperature: 39.3°C (102.7°F)\n",
    "Heart Rate: 131.1/min\n",
    "Blood Pressure: 120/73 mmHg\n",
    "Respiratory Rate: 27.6/min\n",
    "O2 Saturation: 75.8% on room air\n",
    "Weight: 81.2 kg\n",
    "\n",
    "Laboratory/Testing:\n",
    "Comprehensive Respiratory Panel:\n",
    "- Influenza A RNA: Negative\n",
    "- Influenza B RNA: Negative\n",
    "- RSV RNA: Negative\n",
    "- Parainfluenza virus 1,2,3 RNA: Negative\n",
    "- Rhinovirus RNA: Negative\n",
    "- Human metapneumovirus RNA: Negative\n",
    "- Adenovirus DNA: Negative\n",
    "- SARS-CoV-2 RNA: Positive\n",
    "\n",
    "ASSESSMENT:\n",
    "1. Suspected COVID-19 with severe symptoms\n",
    "2. Severe hypoxemia requiring immediate intervention\n",
    "3. Tachycardia (HR 131)\n",
    "4. High-grade fever\n",
    "5. Risk factors:\n",
    "   - Obesity (BMI 33.8)\n",
    "   - Adolescent age\n",
    "\n",
    "PLAN:\n",
    "1. Face mask provided for immediate oxygen support\n",
    "2. Infectious disease care plan initiated\n",
    "3. Close monitoring required due to:\n",
    "   - Severe hypoxemia\n",
    "   - Tachycardia\n",
    "   - Age and obesity risk factors\n",
    "4. Parent/patient education on:\n",
    "   - Home isolation protocols\n",
    "   - Warning signs requiring emergency care\n",
    "   - Return precautions\n",
    "5. Follow-up plan:\n",
    "   - Daily monitoring during acute phase\n",
    "   - Virtual check-ins as needed\n",
    "\n",
    "Encounter Duration: 45 minutes\n",
    "Encounter Type: Ambulatory\n",
    "Provider: ID# e2c226c2-3e1e-3d0b-b997-ce9544c10528\n",
    "Facility: 5103c940-0c08-392f-95cd-446e0cea042a\n",
    "```\n",
    "\n",
    "\n",
    "The enocuter contains\n",
    "\n",
    "* General encounter information: \n",
    "\n",
    "  * When the encounter took place: Date and time of the visit.\n",
    "  * Demographics: Patient’s age, gender, and unique medical record identifier.\n",
    "  * Encounter details: The reason for the visit, diagnosis, and any associated costs.\n",
    "\n",
    "\n",
    "* Semi-Structured Notes:\n",
    "\n",
    "These notes mirror how doctors organize their thoughts and observations during an encounter. They generally follow a SOAP format:\n",
    "\n",
    "* Subjective: The patient’s subjective description of their symptoms, feelings, and medical concerns.\n",
    "* Objective: The doctor’s objective findings, including test results, measurements, or physical examination outcomes.\n",
    "* Assessment: The doctor’s evaluation or diagnosis based on subjective and objective information.\n",
    "* Plan: The proposed treatment plan, including medications, follow-ups, or other interventions.\n",
    "\n",
    "While some encounter notes might include additional details, the majority conform to this semi-structured format, making them ideal for analysis and transformation.\n",
    "\n",
    "* Goals for the Assignment\n",
    "\n",
    "1. Transforming Encounter Notes:\n",
    "\n",
    "Using an LLM to convert semi-structured encounter notes into a JSON format that organizes the information into structured fields. The JSON will include details such as demographics, encounter specifics, and the SOAP components of the note. Subsequently, you will need to transform the JSON data into a Parquet file, which is not only suitable for analysis in Spark but also ideal for storage later.\n",
    "Here we will use the ML classificaition to assing the objective and assessment semi-structured fields into standardized, structured fields. The medical taxonomy for this task will be the one provided by the CDC, which defines standard codes for diagnoses, symptoms, procedures, and treatments. This step ensures the structured data aligns with domain-wide medical standards, making it interoperable and ready for deeper analysis.\n",
    "\n",
    "The JSON format should capture the hierachies described in the structure below. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Basic Analytics and Visualizations:\n",
    "Using Apache Spark, perform comprehensive data analysis on the encounter data and create visualizations that reveal meaningful patterns. Your analysis must include:\n",
    "- COVID-19 Case Demographics: Case breakdown by age ranges ([0-5], [6-10], [11-17], [18-30], [31-50], [51-70], [71+])\n",
    "- Cumulative case count of Covid between the earliest case observed in the dataset and last case observed\n",
    "- Symptoms for all COVID-19 patients versus patients that admitted into the intensive care unit due to COVID.\n",
    "- Rank medications by frequency of prescription\n",
    "- Analyze medication patterns across different demographic groups (e.g., top 3 per age group)\n",
    "- Identify and plot co-morbidity information from the patient records (e.g., hypertension, obesity, prediabetes, etc.) provided in the dataset. \n",
    "- An independent group analysis: You need to develop and execute THREE original analyses that provide meaningful insights about COVID-19 patterns in this dataset. For each analysis:\n",
    "  - Clearly state your analytical question/hypothesis\n",
    "  - Justify why this analysis is valuable\n",
    "  - Show your Spark code and methodology\n",
    "  - Present results with appropriate visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Basemodel, Field, field_validator\n",
    "\n",
    "class Address(BaseModel):\n",
    "    city: str = Field(description=\" the city where the patient lives. Should be under DEMOGRAPHICS header\")\n",
    "    state: str\n",
    "\n",
    "class Demographics(BaseModel):\n",
    "    name: str\n",
    "    date_of_birth: str\n",
    "    age: int\n",
    "    gender: str\n",
    "    address: Address\n",
    "    insurance: str\n",
    "\n",
    "class Medication(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "\n",
    "class PatientRecord(BaseModel):\n",
    "    demographics: Demographics\n",
    "    medications: List[Medication]\n",
    "\n",
    "        \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "structured_llm = llm.with_structured_output(PatientRecord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncounterType:\n",
    "    code\n",
    "    description\n",
    "\n",
    "Encounter:\n",
    "    date\n",
    "    time\n",
    "    provider_id\n",
    "    facility_id\n",
    "\n",
    "Address:\n",
    "    city\n",
    "    state\n",
    "\n",
    "Demographics:\n",
    "    name\n",
    "    date_of_birth\n",
    "    age\n",
    "    gender\n",
    "    address: Address\n",
    "    insurance\n",
    "\n",
    "Condition:\n",
    "    code\n",
    "    description\n",
    "\n",
    "Medication:\n",
    "    code\n",
    "    description\n",
    "\n",
    "Immunization:\n",
    "    code\n",
    "    description\n",
    "    date: date\n",
    "\n",
    "VitalMeasurement:\n",
    "    code\n",
    "    value:\n",
    "    unit\n",
    "\n",
    "BloodPressure:\n",
    "    systolic: VitalMeasurement\n",
    "    diastolic: VitalMeasurement\n",
    "\n",
    "CurrentVitals:\n",
    "    temperature: VitalMeasurement\n",
    "    heart_rate: VitalMeasurement\n",
    "    blood_pressure: BloodPressure\n",
    "    respiratory_rate: VitalMeasurement\n",
    "    oxygen_saturation: VitalMeasurement\n",
    "    weight: VitalMeasurement\n",
    "\n",
    "BaselineVitals:\n",
    "    date: date\n",
    "    height: VitalMeasurement\n",
    "    weight: VitalMeasurement\n",
    "    bmi: VitalMeasurement\n",
    "    bmi_percentile: VitalMeasurement\n",
    "\n",
    "Vitals:\n",
    "    current: CurrentVitals\n",
    "    baseline: BaselineVitals\n",
    "\n",
    "RespiratoryTest:\n",
    "    code\n",
    "    result\n",
    "\n",
    "RespiratoryPanel:\n",
    "    influenza_a: RespiratoryTest\n",
    "    influenza_b: RespiratoryTest\n",
    "    rsv: RespiratoryTest\n",
    "    parainfluenza_1: RespiratoryTest\n",
    "    parainfluenza_2: RespiratoryTest\n",
    "    parainfluenza_3: RespiratoryTest\n",
    "    rhinovirus: RespiratoryTest\n",
    "    metapneumovirus: RespiratoryTest\n",
    "    adenovirus: RespiratoryTest\n",
    "\n",
    "Covid19Test:\n",
    "    code\n",
    "    description\n",
    "    result\n",
    "\n",
    "Laboratory:\n",
    "    covid19: Covid19Test\n",
    "    respiratory_panel: RespiratoryPanel\n",
    "\n",
    "Procedure:\n",
    "    code\n",
    "    description\n",
    "    date: date\n",
    "    reasonCode\n",
    "    reasonDescription\n",
    "\n",
    "CarePlan:\n",
    "    code\n",
    "    description\n",
    "    start: date\n",
    "    stop: date\n",
    "    reasonCode\n",
    "    reasonDescription\n",
    "\n",
    "PatientRecord:\n",
    "    encounter: Encounter\n",
    "    demographics: Demographics\n",
    "    conditions: List[Condition]\n",
    "    medications: List[Medication]\n",
    "    immunizations: List[Immunization]\n",
    "    vitals: Vitals\n",
    "    laboratory: Laboratory\n",
    "    procedures: List[Procedure]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transforming Encounter Notes:\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "from pydantic import ValidationError\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Load CSV files\n",
    "encounters_df = pd.read_csv(\"data/encounters_assignment_1.csv\")\n",
    "encounter_types_df = pd.read_csv(\"data/encounters_types_assignment_1.csv\")\n",
    "immunizations_df = pd.read_csv(\"data/immunizations_assignment_1.csv\")\n",
    "medications_df = pd.read_csv(\"data/medications_assignment_1.csv\")\n",
    "observations_df = pd.read_csv(\"data/observations_assignment_1.csv\")\n",
    "\n",
    "# Inspect the data\n",
    "print(\"Encounters Data:\")\n",
    "print(encounters_df.head())\n",
    "print(\"\\nEncounter Types Data:\")\n",
    "print(encounter_types_df.head())\n",
    "print(\"\\nImmunizations Data:\")\n",
    "print(immunizations_df.head())\n",
    "print(\"\\nMedications Data:\")\n",
    "print(medications_df.head())\n",
    "print(\"\\nObservations Data:\")\n",
    "print(observations_df.head())\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the path to the directory containing encounter notes\n",
    "notes_dir = r\"data/encounter_notes\"\n",
    "\n",
    "# List all text files in the directory\n",
    "text_files = [os.path.join(notes_dir, f) for f in os.listdir(notes_dir) if f.endswith('.txt')]\n",
    "print(\"Text Files Found:\", text_files)\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class Address(BaseModel):\n",
    "    city: str = Field(description=\"City of residence.\")\n",
    "    state: str = Field(description=\"State of residence.\")\n",
    "    postal_code: Optional[str] = Field(None, description=\"Postal code.\")\n",
    "\n",
    "class Demographics(BaseModel):\n",
    "    name: str = Field(description=\"Full name of the patient.\")\n",
    "    dob: str = Field(description=\"Date of birth in MM/DD/YYYY format.\")\n",
    "    age: int = Field(description=\"Calculated age of the patient in years.\")\n",
    "    gender: str = Field(description=\"Gender of the patient.\")\n",
    "    address: Address = Field(description=\"Address details.\")\n",
    "    insurance: str = Field(description=\"Insurance information.\")\n",
    "    mrn: str = Field(description=\"Unique medical record number.\")\n",
    "\n",
    "    @field_validator(\"dob\")\n",
    "    def validate_dob_format(cls, value):\n",
    "        try:\n",
    "            datetime.strptime(value, \"%m/%d/%Y\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Date of Birth must be in MM/DD/YYYY format.\")\n",
    "        return value\n",
    "\n",
    "class Vitals(BaseModel):\n",
    "    temperature: Optional[float] = Field(None, description=\"Body temperature in Celsius.\")\n",
    "    heart_rate: Optional[float] = Field(None, description=\"Heart rate in beats per minute.\")\n",
    "    blood_pressure: Optional[str] = Field(None, description=\"Blood pressure (e.g., 120/80).\")\n",
    "    respiratory_rate: Optional[float] = Field(None, description=\"Respiratory rate.\")\n",
    "    o2_saturation: Optional[float] = Field(None, description=\"Oxygen saturation.\")\n",
    "\n",
    "class SOAP(BaseModel):\n",
    "    subjective: List[str] = Field(description=\"Subjective symptoms described by the patient.\")\n",
    "    objective: Vitals = Field(description=\"Objective findings.\")\n",
    "    assessment: List[str] = Field(description=\"Doctor's evaluation.\")\n",
    "    plan: List[str] = Field(description=\"Proposed treatment plan.\")\n",
    "\n",
    "class Encounter(BaseModel):\n",
    "    encounter_note: str = Field(description=\"Full text of the encounter note.\")\n",
    "    date_of_service: datetime = Field(description=\"Date and time of the encounter.\")\n",
    "    demographics: Demographics = Field(description=\"Demographic information.\")\n",
    "    soap: SOAP = Field(description=\"SOAP note details.\")\n",
    "    provider_id: str = Field(description=\"Provider's unique identifier.\")\n",
    "    facility_id: Optional[str] = Field(None, description=\"Facility identifier.\")\n",
    "    encounter_duration: Optional[int] = Field(None, description=\"Duration in minutes.\")\n",
    "    encounter_type: str = Field(description=\"Type of encounter (e.g., Urgent Care).\")\n",
    "\n",
    "\n",
    "\n",
    "openai.api_key = \"Secret!\"\n",
    "\n",
    "# Define the OpenAI prompt generator\n",
    "def generate_openai_prompt(note: str) -> str:\n",
    "    schema_description = \"\"\"\n",
    "    Your task is to extract structured information from unstructured medical encounter notes using the schema below. \n",
    "    - For the demographics section, calculate the patient's age in years based on the Date of Birth (DOB) and Date of Service (DOS). \n",
    "    - Ensure the Date of Birth (dob) is strictly in MM/DD/YYYY format.\n",
    "    - Ensure `date_of_service` is formatted as `YYYY-MM-DD HH:MM:SS`.\n",
    "    - Include the calculated age under the demographics section.\n",
    "    - Ensure the output is a valid JSON object strictly following the schema.\n",
    "\n",
    "    JSON Schema:\n",
    "    {\n",
    "        \"encounter_note\": \"string - Full text of the encounter note.\",\n",
    "        \"date_of_service\": \"datetime - Date and time of the encounter.\",\n",
    "        \"demographics\": {\n",
    "            \"name\": \"string - Full name of the patient.\",\n",
    "            \"dob\": \"string - Patient's date of birth in MM/DD/YYYY format.\",\n",
    "            \"age\": \"int - Patient's age in years, calculated from DOB and DOS.\",\n",
    "            \"gender\": \"string - Gender of the patient (e.g., Male, Female).\",\n",
    "            \"address\": {\n",
    "                \"city\": \"string - City of residence.\",\n",
    "                \"state\": \"string - State of residence.\",\n",
    "                \"postal_code\": \"string - Postal code (if available).\"\n",
    "            },\n",
    "            \"insurance\": \"string - Name of the insurance provider.\",\n",
    "            \"mrn\": \"string - Unique medical record number.\"\n",
    "        },\n",
    "        \"soap\": {\n",
    "            \"subjective\": [\"string - List of symptoms described by the patient.\"],\n",
    "            \"objective\": {\n",
    "                \"vitals\": {\n",
    "                    \"temperature\": \"float - Body temperature in Celsius.\",\n",
    "                    \"heart_rate\": \"float - Heart rate in beats per minute.\",\n",
    "                    \"blood_pressure\": \"string - Blood pressure in mmHg (e.g., 120/80).\",\n",
    "                    \"respiratory_rate\": \"float - Respiratory rate in breaths per minute.\",\n",
    "                    \"o2_saturation\": \"float - Oxygen saturation percentage.\"\n",
    "                }\n",
    "            },\n",
    "            \"assessment\": [\"string - List of diagnoses or evaluations.\"],\n",
    "            \"plan\": [\"string - List of proposed treatments or next steps.\"]\n",
    "        },\n",
    "        \"provider_id\": \"string - Unique identifier of the healthcare provider.\",\n",
    "        \"facility_id\": \"string - Unique identifier of the facility.\",\n",
    "        \"encounter_duration\": \"int - Duration of the encounter in minutes.\",\n",
    "        \"encounter_type\": \"string - Type of encounter (e.g., Urgent Care, Ambulatory).\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract structured information from the following medical encounter note. Ensure the data strictly adheres to the JSON schema described above. \n",
    "    - Calculate the patient's age in years by subtracting the year of birth (DOB) from the year of the Date of Service (DOS). \n",
    "    - Ensure all fields are present, even if optional fields (like postal code) are null. \n",
    "\n",
    "    Encounter Note:\n",
    "    {note}\n",
    "    \"\"\"\n",
    "    return schema_description + prompt\n",
    "\n",
    "# Parse encounter notes using OpenAI\n",
    "def parse_encounter_notes(note: str) -> dict:\n",
    "    prompt = generate_openai_prompt(note)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical assistant skilled at extracting structured data from medical notes.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        # Convert the JSON string output to a Python dictionary\n",
    "        return json.loads(response['choices'][0]['message']['content'])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call: {e}\")\n",
    "        return None\n",
    "\n",
    "# Preprocess the date_of_service field (for preventing validation errors)\n",
    "def preprocess_date_of_service(json_data: dict) -> dict:\n",
    "    try:\n",
    "        if \"date_of_service\" in json_data:\n",
    "            raw_date = json_data[\"date_of_service\"]\n",
    "            # If already in ISO format, skip further processing\n",
    "            if \"T\" in raw_date or \"-\" in raw_date:\n",
    "                return json_data\n",
    "            # Parse and reformat non-standard datetime\n",
    "            parsed_date = datetime.strptime(raw_date.split(\"-\")[0], \"%B %d, %Y %H:%M\")\n",
    "            json_data[\"date_of_service\"] = parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during date_of_service preprocessing: {e}\")\n",
    "    return json_data\n",
    "\n",
    "# Directory containing encounter notes\n",
    "notes_dir = r\"data/encounter_notes\"\n",
    "text_files = [os.path.join(notes_dir, f) for f in os.listdir(notes_dir) if f.endswith('.txt')]\n",
    "\n",
    "# Process and validate encounter notes\n",
    "validated_encounters = []\n",
    "\n",
    "for file_path in text_files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_note = f.read()\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        parsed_note = parse_encounter_notes(raw_note)\n",
    "        if parsed_note:\n",
    "            try:\n",
    "                # Preprocess the parsed JSON data\n",
    "                parsed_json = preprocess_date_of_service(parsed_note)\n",
    "                # Validate the preprocessed data using Pydantic\n",
    "                validated_data = Encounter.model_validate(parsed_json)\n",
    "                validated_encounters.append(validated_data.model_dump())\n",
    "            except ValidationError as e:\n",
    "                print(f\"Validation Error for {file_path}:\", e.json(indent=2))\n",
    "\n",
    "# Save validated data to Parquet\n",
    "validated_df = pd.DataFrame(validated_encounters)\n",
    "parquet_path = r\"data/merged_encounter_data.parquet\"\n",
    "table = pa.Table.from_pandas(validated_df)\n",
    "pq.write_table(table, parquet_path)\n",
    "\n",
    "print(\"Data successfully saved to Parquet.\")\n",
    "\n",
    "# Text Files Found: ['data/encounter_notes\\\\055ae6fc-7e18-4a39-8058-64082ca6d515.txt', 'data/encounter_notes\\\\199c586f-af16-4091-9998-ee4cfc02ee7a.txt', 'data/encounter_notes\\\\28658715-b770-4576-9a81-fbb2282a98ea.txt', 'data/encounter_notes\\\\353016ea-a0ff-4154-85bb-1cf8b6cedf20.txt', 'data/encounter_notes\\\\ae9efba3-ddc4-43f9-a781-f72019388548.txt', 'data/encounter_notes\\\\b9fd2dd8-181b-494b-ab15-e9f286d668d9.txt', 'data/encounter_notes\\\\d22592ac-552f-4ecd-a63d-7663d77ce9ba.txt', 'data/encounter_notes\\\\df6b563d-1ff4-4833-9af8-84431e641e9c.txt', 'data/encounter_notes\\\\f0f3bc8d-ef38-49ce-a2bd-dfdda982b271.txt', 'data/encounter_notes\\\\f73d6f41-0091-4485-8b43-9d38eb98fb36.txt']\n",
    "# Processing: data/encounter_notes\\055ae6fc-7e18-4a39-8058-64082ca6d515.txt    \n",
    "# Processing: data/encounter_notes\\199c586f-af16-4091-9998-ee4cfc02ee7a.txt    \n",
    "# Processing: data/encounter_notes\\28658715-b770-4576-9a81-fbb2282a98ea.txt    \n",
    "# JSON Decode Error: Expecting value: line 1 column 1 (char 0)\n",
    "# Processing: data/encounter_notes\\353016ea-a0ff-4154-85bb-1cf8b6cedf20.txt    \n",
    "# Processing: data/encounter_notes\\ae9efba3-ddc4-43f9-a781-f72019388548.txt\n",
    "# Processing: data/encounter_notes\\b9fd2dd8-181b-494b-ab15-e9f286d668d9.txt\n",
    "# Processing: data/encounter_notes\\d22592ac-552f-4ecd-a63d-7663d77ce9ba.txt\n",
    "# Processing: data/encounter_notes\\df6b563d-1ff4-4833-9af8-84431e641e9c.txt\n",
    "# Processing: data/encounter_notes\\f0f3bc8d-ef38-49ce-a2bd-dfdda982b271.txt\n",
    "# Processing: data/encounter_notes\\f73d6f41-0091-4485-8b43-9d38eb98fb36.txt\n",
    "# Data successfully saved to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided JSON and turn it into aparquet file\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "jsonl_path = r\"Parsed_notes.jsonl\"\n",
    "\n",
    "# Read JSONL into a list of dictionaries\n",
    "parsed_notes = []\n",
    "with open(jsonl_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parsed_notes.append(json.loads(line))\n",
    "\n",
    "parsed_df = pd.json_normalize(parsed_notes)\n",
    "print(parsed_df.head())\n",
    "\n",
    "parquet_path = r\"new_parsed_notes.parquet\"\n",
    "\n",
    "table = pa.Table.from_pandas(parsed_df)\n",
    "pq.write_table(table, parquet_path)\n",
    "\n",
    "print(f\"Data successfully saved to Parquet at {parquet_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Basic Analytics and Visualizations: (Code from databricks)\n",
    "\n",
    "parquet_path = \"dbfs:/FileStore/new_parsed_notes.parquet\"\n",
    "\n",
    "df = spark.read.parquet(parquet_path)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"COVID-19 Data Analytics Using Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "parsed_notes_parquet = \"dbfs:/FileStore/new_parsed_notes.parquet\"\n",
    "parsed_notes_df = spark.read.parquet(parsed_notes_parquet)\n",
    "\n",
    "# parsed_notes_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 Case Demographics\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "age_ranges = [\n",
    "    (0, 5), (6, 10), (11, 17), (18, 30), (31, 50), (51, 70), (71, 120)\n",
    "]\n",
    "\n",
    "def age_range(age):\n",
    "    for start, end in age_ranges:\n",
    "        if start <= age <= end:\n",
    "            return f\"{start}-{end}\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "age_range_udf = udf(age_range, StringType())\n",
    "\n",
    "demographics_by_age = parsed_notes_df \\\n",
    "    .withColumn(\"age_range\", age_range_udf(col(\"`demographics.age`\"))) \\\n",
    "    .groupBy(\"age_range\").count() \\\n",
    "    .orderBy(\"age_range\")\n",
    "\n",
    "demographics_by_age.show()\n",
    "\n",
    "# +---------+-----+\n",
    "# |age_range|count|\n",
    "# +---------+-----+\n",
    "# |      0-5|   25|\n",
    "# |    11-17|  126|\n",
    "# |    18-30|  320|\n",
    "# |    31-50|  436|\n",
    "# |    51-70|  508|\n",
    "# |     6-10|   95|\n",
    "# |   71-120|  489|\n",
    "# +---------+-----+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Case Count\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "cumulative_cases = parsed_notes_df \\\n",
    "    .withColumn(\"case_date\", to_date(col(\"`encounter.date`\"))) \\\n",
    "    .groupBy(\"case_date\").count() \\\n",
    "    .orderBy(\"case_date\")\n",
    "\n",
    "cumulative_cases.show(10)\n",
    "\n",
    "\n",
    "# +----------+-----+\n",
    "# | case_date|count|\n",
    "# +----------+-----+\n",
    "# |      null|  398|\n",
    "# |2020-01-01|    1|\n",
    "# |2020-01-04|    1|\n",
    "# |2020-01-11|    1|\n",
    "# |2020-01-12|    1|\n",
    "# |2020-01-13|    1|\n",
    "# |2020-01-15|    1|\n",
    "# |2020-01-16|    2|\n",
    "# |2020-01-17|    1|\n",
    "# |2020-01-20|    2|\n",
    "# +----------+-----+\n",
    "# only showing top 10 rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symptoms Analysis\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "symptoms_analysis = parsed_notes_df \\\n",
    "    .withColumn(\"condition_code\", explode(col(\"conditions.code\"))) \\\n",
    "    .groupBy(\"condition_code\").count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "symptoms_analysis.show(10)\n",
    "\n",
    "\n",
    "# +--------------+-----+\n",
    "# |condition_code|count|\n",
    "# +--------------+-----+\n",
    "# |     840544004| 1486|\n",
    "# |     840539006| 1435|\n",
    "# |     386661006| 1322|\n",
    "# |      49727002| 1028|\n",
    "# |      36955009|  747|\n",
    "# |      84229001|  560|\n",
    "# |     248595008|  476|\n",
    "# |     233604007|  303|\n",
    "# |     271825005|  302|\n",
    "# |     389087006|  302|\n",
    "# +--------------+-----+\n",
    "# only showing top 10 rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank Medications by Frequency\n",
    "\n",
    "top_medications = parsed_notes_df \\\n",
    "    .withColumn(\"medication_code\", explode(col(\"medications.code\"))) \\\n",
    "    .groupBy(\"medication_code\").count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "top_medications.show(10)\n",
    "\n",
    "# +---------------+-----+\n",
    "# |medication_code|count|\n",
    "# +---------------+-----+\n",
    "# |         198440|  302|\n",
    "# |         854235|  291|\n",
    "# |         205923|  214|\n",
    "# |         854252|  174|\n",
    "# |        2123111|  165|\n",
    "# |         106892|   88|\n",
    "# |         314231|   83|\n",
    "# |         310798|   82|\n",
    "# |         860975|   69|\n",
    "# |         999967|   52|\n",
    "# +---------------+-----+\n",
    "# only showing top 10 rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Medication Patterns Across Age Groups\n",
    "\n",
    "medications_by_age = parsed_notes_df \\\n",
    "    .withColumn(\"age_range\", age_range_udf(col(\"`demographics.age`\"))) \\\n",
    "    .withColumn(\"medication_code\", explode(col(\"medications.code\"))) \\\n",
    "    .groupBy(\"age_range\", \"medication_code\").count() \\\n",
    "    .orderBy(\"age_range\", col(\"count\").desc())\n",
    "\n",
    "medications_by_age.show(10)\n",
    "\n",
    "# +---------+---------------+-----+\n",
    "# |age_range|medication_code|count|\n",
    "# +---------+---------------+-----+\n",
    "# |      0-5|         198405|    3|\n",
    "# |      0-5|         854252|    1|\n",
    "# |      0-5|        2123111|    1|\n",
    "# |      0-5|         854235|    1|\n",
    "# |      0-5|        1043400|    1|\n",
    "# |      0-5|         308182|    1|\n",
    "# |      0-5|         198440|    1|\n",
    "# |    11-17|        2123111|   14|\n",
    "# |    11-17|         895994|   11|\n",
    "# |    11-17|         854235|    9|\n",
    "# +---------+---------------+-----+\n",
    "# only showing top 10 rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze co-morbidities from conditions\n",
    "\n",
    "co_morbidities = parsed_notes_df \\\n",
    "    .withColumn(\"condition_code\", explode(col(\"conditions.code\"))) \\\n",
    "    .groupBy(\"condition_code\").count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "co_morbidities.show(10)\n",
    "\n",
    "# +--------------+-----+\n",
    "# |condition_code|count|\n",
    "# +--------------+-----+\n",
    "# |     840544004| 1486|\n",
    "# |     840539006| 1435|\n",
    "# |     386661006| 1322|\n",
    "# |      49727002| 1028|\n",
    "# |      36955009|  747|\n",
    "# |      84229001|  560|\n",
    "# |     248595008|  476|\n",
    "# |     233604007|  303|\n",
    "# |     271825005|  302|\n",
    "# |     389087006|  302|\n",
    "# +--------------+-----+\n",
    "# only showing top 10 rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 symptoms in Covid-19 cases\n",
    "\n",
    "### Analytical Question / Hypothesis:\n",
    "What are the top 5 symptoms in COVID-19? \n",
    "The most common symptoms reported by COVID-19 cases in the dataset? Identifying common symptoms can help healthcare providers prioritize diagnostic criteria and treatment plans.\n",
    "\n",
    "### Justification:\n",
    "Understanding the most frequently reported symptoms provides insights into COVID-19. This enables public awareness of the disease and helps create a plan to prevent the spread of pathogens based on the symptoms.\n",
    "\n",
    "### Spark Code and Methodology:\n",
    "The dataset was processed to explode the conditions column and group by symptom codes to count their occurrences. The top 5 symptoms were visualized using a pie chart.\n",
    "\n",
    "### Result / Visualization:\n",
    "The most common symptoms in COVID-19 cases were: 1. cough, 2. fever, and 3. shortness of breath, which matches the order of possible symptoms listed by the Centers for Disease Control and Prevention (CDC).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, lit, when\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a mapping of SNOMED CT codes to their descriptions\n",
    "snomed_ct_mapping = {\n",
    "    \"386661006\": \"Fever\",\n",
    "    \"49727002\": \"Cough\",\n",
    "    \"36955009\": \"Shortness of breath\",\n",
    "    \"233604007\": \"Sore throat\",\n",
    "    \"389087006\": \"Loss of sense of smell\",\n",
    "    \"271825005\": \"Loss of sense of taste\"\n",
    "}\n",
    "\n",
    "# List of codes to exclude\n",
    "exclude_codes = [\"840544004\", \"840539006\", \"84229001\", \"248595008\"]\n",
    "\n",
    "symptoms_prevalence = (\n",
    "    parsed_notes_df\n",
    "    .withColumn(\"symptom_code\", explode(col(\"conditions.code\")))  # Explode the symptom codes\n",
    "    .filter(~col(\"symptom_code\").isin(*exclude_codes))  # Exclude non-symptom codes\n",
    "    .groupBy(\"symptom_code\")\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc())\n",
    ")\n",
    "\n",
    "# Replace codes with descriptions for clarity\n",
    "symptoms_prevalence = symptoms_prevalence.withColumn(\n",
    "    \"symptom_description\",\n",
    "    lit(None).cast(\"string\")\n",
    ")\n",
    "\n",
    "# Map symptom codes to descriptions\n",
    "for code, description in snomed_ct_mapping.items():\n",
    "    symptoms_prevalence = symptoms_prevalence.withColumn(\n",
    "        \"symptom_description\",\n",
    "        when(col(\"symptom_code\") == code, lit(description)).otherwise(col(\"symptom_description\"))\n",
    "    )\n",
    "\n",
    "symptoms_prevalence = symptoms_prevalence.filter(col(\"symptom_description\").isNotNull())\n",
    "symptoms_prevalence_pd = symptoms_prevalence.toPandas()\n",
    "top_symptoms = symptoms_prevalence_pd.head(5)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    top_symptoms[\"count\"],\n",
    "    labels=top_symptoms[\"symptom_description\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140\n",
    ")\n",
    "plt.title(\"Top 5 Symptoms in COVID-19 Cases\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Debug_and_etc/Top_5_Symptoms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3 Medications Prescribed in Each Age Group\n",
    "\n",
    "### Analytical Question/Hypothesis:\n",
    "What are the most commonly prescribed medications for each age group of COVID-19 patients?\n",
    "We hypothesize that medication patterns vary across age groups due to differences in treatment protocols and patient needs.\n",
    "\n",
    "### Justification:\n",
    "The analysis aims to highlight medication trends and support the understanding of age-specific therapeutic interventions. The visualization could help guide better pharmaceutical management, such as stocking specific medications.\n",
    "\n",
    "### Spark Code and Methodology:\n",
    "The medications column was used to list individual prescriptions. Medications were grouped by age range and ordered by frequency, and the top 3 medications for each group were visualized.\n",
    "\n",
    "### Result / Visualization:\n",
    "The result makes sense because, for example, the most commonly used medication among those aged 18-70 was acetaminophen 500 mg oral tablet. This medication is used for reducing fever, which matches the most common symptoms identified in the previous analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, first\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "medications_mapping_path = \"dbfs:/FileStore/medications_assignment_1.csv\"\n",
    "medications_mapping_df = spark.read.option(\"header\", \"true\").csv(medications_mapping_path)\n",
    "\n",
    "medications_mapping_pd = medications_mapping_df.toPandas()\n",
    "medication_mapping = medications_mapping_pd.groupby(\"CODE\")[\"DESCRIPTION\"].first().to_dict()\n",
    "\n",
    "# Replace medication codes with descriptions in medications_by_age\n",
    "medications_by_age_pd = medications_by_age.toPandas()\n",
    "medications_by_age_pd[\"medication_description\"] = medications_by_age_pd[\"medication_code\"].map(medication_mapping)\n",
    "\n",
    "age_groups = medications_by_age_pd[\"age_range\"].unique()\n",
    "n_age_groups = len(age_groups)\n",
    "\n",
    "n_cols = 1\n",
    "n_rows = math.ceil(n_age_groups / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 10), constrained_layout=True)\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, age_group in enumerate(age_groups):\n",
    "    # Filter data for the current age group and select top 3 medications\n",
    "    age_group_data = medications_by_age_pd[medications_by_age_pd[\"age_range\"] == age_group].head(3)\n",
    "    \n",
    "    axes[idx].barh(age_group_data[\"medication_description\"], age_group_data[\"count\"], color=\"skyblue\")\n",
    "    axes[idx].set_title(f\"Top 3 Medications for Age Group: {age_group}\", fontsize=12)\n",
    "    axes[idx].set_xlabel(\"Number of Prescriptions\", fontsize=10)\n",
    "    axes[idx].set_ylabel(\"Medication Name\", fontsize=10)\n",
    "    axes[idx].tick_params(axis='y', labelsize=8) \n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "for idx in range(n_age_groups, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "fig.suptitle(\"Top 3 Medications Prescribed in Each Age Group\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Debug_and_etc/Top_3_Medications.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Spent in Encounters by Age Group\n",
    "\n",
    "## Analytical Question/Hypothesis:\n",
    "Do encounter durations vary significantly among different age groups?\n",
    "Older patients may require longer encounters due to comorbidities or complexity in care.\n",
    "\n",
    "## Justification:\n",
    "Understanding time spent in encounters by age group aids in resource planning and allocation in healthcare settings, helps hospital time management during outbreak, ensuring equitable care delivery.\n",
    "\n",
    "## Spark Code and Methodology:\n",
    "Encounter durations were aggregated by age range to compute total and average times. The results were visualized using a simple bar chart.\n",
    "\n",
    "## Result / Visualization:\n",
    "The result shows a trend that the older the age group, the more time spent in encounters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parsed_notes_df = parsed_notes_df.withColumn(\n",
    "    \"encounter_datetime\",\n",
    "    unix_timestamp(concat_ws(\" \", col(\"`encounter.date`\"), col(\"`encounter.time`\")), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "# If encounter duration is available, use it. Otherwise, simulate a fixed duration.\n",
    "parsed_notes_df = parsed_notes_df.withColumn(\n",
    "    \"encounter_duration\",\n",
    "    when(col(\"encounter_datetime\").isNotNull(), 30).otherwise(0)  # Replace `30` with actual logic if available\n",
    ")\n",
    "\n",
    "# Define age ranges\n",
    "parsed_notes_df = parsed_notes_df.withColumn(\n",
    "    \"age_range\",\n",
    "    when((col(\"`demographics.age`\") >= 0) & (col(\"`demographics.age`\") <= 5), \"0-5\")\n",
    "    .when((col(\"`demographics.age`\") >= 6) & (col(\"`demographics.age`\") <= 10), \"6-10\")\n",
    "    .when((col(\"`demographics.age`\") >= 11) & (col(\"`demographics.age`\") <= 17), \"11-17\")\n",
    "    .when((col(\"`demographics.age`\") >= 18) & (col(\"`demographics.age`\") <= 30), \"18-30\")\n",
    "    .when((col(\"`demographics.age`\") >= 31) & (col(\"`demographics.age`\") <= 50), \"31-50\")\n",
    "    .when((col(\"`demographics.age`\") >= 51) & (col(\"`demographics.age`\") <= 70), \"51-70\")\n",
    "    .when(col(\"`demographics.age`\") > 70, \"71-120\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Add a numerical index for sorting\n",
    "parsed_notes_df = parsed_notes_df.withColumn(\n",
    "    \"age_range_order\",\n",
    "    when(col(\"age_range\") == \"0-5\", 1)\n",
    "    .when(col(\"age_range\") == \"6-10\", 2)\n",
    "    .when(col(\"age_range\") == \"11-17\", 3)\n",
    "    .when(col(\"age_range\") == \"18-30\", 4)\n",
    "    .when(col(\"age_range\") == \"31-50\", 5)\n",
    "    .when(col(\"age_range\") == \"51-70\", 6)\n",
    "    .when(col(\"age_range\") == \"71-120\", 7)\n",
    ")\n",
    "\n",
    "# Aggregate durations by age range\n",
    "encounter_duration_by_age = parsed_notes_df \\\n",
    "    .groupBy(\"age_range\", \"age_range_order\") \\\n",
    "    .agg(\n",
    "        round(sum(\"encounter_duration\"), 1).alias(\"total_time_spent (min)\"),\n",
    "        round(avg(\"encounter_duration\"), 1).alias(\"average_time_spent (min)\")\n",
    "    ) \\\n",
    "    .orderBy(\"age_range_order\")\n",
    "\n",
    "# Drop helper column for display\n",
    "encounter_duration_by_age = encounter_duration_by_age.drop(\"age_range_order\")\n",
    "\n",
    "# Show the result\n",
    "encounter_duration_by_age.show()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "encounter_duration_by_age_pd = encounter_duration_by_age.toPandas()\n",
    "\n",
    "# Visualization: Bar chart for average time spent\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(encounter_duration_by_age_pd[\"age_range\"], encounter_duration_by_age_pd[\"average_time_spent (min)\"], color=\"skyblue\")\n",
    "plt.title(\"Average Time Spent in Encounters by Age Group (Minutes)\", fontsize=14)\n",
    "plt.xlabel(\"Age Group\", fontsize=12)\n",
    "plt.ylabel(\"Average Time Spent (Minutes)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# +---------+----------------------+------------------------+\n",
    "# |age_range|total_time_spent (min)|average_time_spent (min)|\n",
    "# +---------+----------------------+------------------------+\n",
    "# |      0-5|                   750|                    30.0|\n",
    "# |     6-10|                  2700|                    28.4|\n",
    "# |    11-17|                  3480|                    27.6|\n",
    "# |    18-30|                  8940|                    27.9|\n",
    "# |    31-50|                 12120|                    27.8|\n",
    "# |    51-70|                 13170|                    25.9|\n",
    "# |   71-120|                  6870|                    14.0|\n",
    "# +---------+----------------------+------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Debug_and_etc/average_time_spent.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Assignment 2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
